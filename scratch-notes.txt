# Scratch Notes — Translation Wiki Project Planning

## Research Summary

### Tech Stack Confirmed
- Next.js 15+ (App Router, Turbopack) — `npx create-next-app@latest --typescript --tailwind --eslint --app --src-dir`
- tRPC v11 with @trpc/tanstack-react-query — fetch adapter for App Router API routes
- Drizzle ORM — relations v1 API (stable), v2 in beta
- NextAuth.js v5 (Auth.js) — credentials provider + OAuth, JWT strategy
- Shadcn UI — `npx shadcn@latest init` then add components as needed
- Vitest + @testing-library/react — jsdom environment, vite-tsconfig-paths
- PostgreSQL — Neon (free tier: 0.5GB, 190 compute hours) or local Docker

### Key Technical Decisions

1. **Package manager:** pnpm recommended (better shadcn compatibility with React 19)
2. **tRPC v11 pattern:**
   - /trpc folder with init.ts, router.ts, client.tsx, server.tsx
   - fetchRequestHandler at /api/trpc/[trpc]/route.ts
   - createCaller for server components (no HTTP overhead)
   - prefetch + HydrateClient for streaming
3. **Auth pattern:**
   - auth.config.ts (providers) + auth.ts (adapter + session strategy)
   - app/api/auth/[...nextauth]/route.ts exports { GET, POST }
   - JWT strategy (no database session table needed initially)
   - Middleware for route protection
4. **Drizzle pattern:**
   - pgTable definitions with explicit column types
   - relations() for ORM-level relationships
   - .references() for FK constraints
   - drizzle-kit for migrations (drizzle.config.ts)
5. **Testing:**
   - Vitest + jsdom for unit/component tests
   - Playwright for E2E (async server components can't be tested with Vitest)
   - vitest.config.mts at root

### Source Text Details

**Zhu Zi Yu Lei (朱子語類)**
- 140 chapters (juan/卷)
- URN: ctp:zhuzi-yulei
- Individual chapters: ctp:zhuzi-yulei/1 through /140
- Each chapter returns `fulltext` (ordered paragraph list)
- Topics: metaphysics, learning, Four Books commentary, classics, history, misc
- API requires authentication for subsections enumeration
- Python `ctext` package available, or raw JSON API calls

**De Ceremoniis**
- 2 books, ~180+ chapters total
- Internet Archive: archive.org/details/bub_gb_OFpFAAAAYAAJ (Reiske edition)
- Greek text + Latin translation in same edition
- OCR quality varies — polytonic Greek diacritics are problematic
- Princeton Byzantine Translations may have partial cleaner text
- Moffatt/Tall 2012 English translation is under copyright — cannot use
- Need AI-generated English translation from the Greek

### Data Model Notes

Content storage: JSON column with paragraph arrays
- Source: { paragraphs: [{ index: 0, text: "..." }, ...] }
- Translation mirrors with matching indices
- This enables paragraph-aligned rendering in InterlinearViewer
- For Chinese: each "paragraph" might be a single statement or exchange
- For Greek: each "paragraph" might be a section or stanza

Versioning:
- TranslationVersion is append-only
- Each version stores full content (not diffs) — simplifies reading, costs more storage
- Diffs computed on-the-fly for display using a library like `diff` or `jsdiff`
- currentVersionId on Translation points to latest

Endorsements:
- One per user per version (unique constraint)
- Count aggregated for display
- Version with most endorsements = "community preferred"

### Interlinear Viewer Design Notes

Core concept: CSS Grid or Flexbox table with two columns
- Each row = one ParagraphPair component
- Left cell: source text (potentially with original script font)
- Right cell: translation text
- Rows auto-height to tallest cell (natural CSS grid behaviour)
- Synchronized scroll NOT needed if using row-based alignment
- Mobile: stack vertically (source above, translation below)
- Font considerations:
  - Chinese: Noto Serif CJK or Source Han Serif
  - Greek: Noto Serif with polytonic support
  - Latin: any good serif
  - English translation: system font stack or Inter

### Deployment Architecture

MVP path (Vercel + Neon):
- Vercel Hobby (free) handles Next.js deployment
- Neon free tier for PostgreSQL
- Vercel automatically builds on git push
- Environment variables in Vercel dashboard
- Custom domain via Vercel DNS or external registrar

Production path (self-hosted):
- Docker Compose: Next.js container + PostgreSQL container
- Caddy or nginx as reverse proxy with auto-SSL
- VPS: Hetzner CAX11 (€3.79/mo, ARM) or DigitalOcean $6/mo
- Backups: pg_dump cron to object storage

### Implementation Order (Dependencies)

Phase 1 must come first: schema, auth, Docker compose
Phase 2 can parallel with Phase 3 partially
Phase 3 depends on seeded data (Phase 2)
Phase 4 depends on Phase 3 (reading UI exists)
Phase 5 depends on Phase 4 (versions exist to endorse)
Phase 6 can happen any time after Phase 2
Phase 7 can happen after Phase 3
Phase 8 can happen after Phase 1 (deploy empty, then iterate)

Actually, earliest useful deployment is after Phase 3 — reading works.

### Risks and Mitigations

1. ctext.org rate limits / auth requirements
   - Mitigation: create account, cache aggressively, store raw in data/raw/
   - Fallback: manually download chapter pages and parse HTML

2. OCR quality of De Ceremoniis
   - Mitigation: Princeton Byzantine Translations for partial clean text
   - Mitigation: AI-assisted cleanup of OCR output
   - Mitigation: start with a few chapters, iterate

3. Paragraph alignment between source and translation
   - Mitigation: editor enforces paragraph count matching
   - Mitigation: allow "empty" paragraphs for padding

4. Polytonic Greek rendering
   - Mitigation: test with Noto Serif early, specify @font-face
   - Mitigation: Unicode normalization (NFC) on input

5. Scale of Zhu Zi Yu Lei (140 chapters, potentially thousands of paragraphs)
   - Mitigation: paginate by chapter (already planned)
   - Mitigation: lazy-load translation versions

### Translation API Observations (Session 2)

**DeepSeek R1 (deepseek-reasoner):**
- Very slow — each batch takes 30-60+ seconds due to chain-of-thought reasoning
- Unnecessary for translation (reasoning overhead provides no benefit)
- 64K max output — would eliminate batching need, but too slow to be practical
- Sometimes returns string indices instead of numbers (needed coercion fix)

**DeepSeek V3.2 (deepseek-chat):**
- Much faster — responses in seconds
- 8K max output (hard limit enforced by API: "valid range is [1, 8192]")
- Requires paragraph batching for large chapters
- $0.28/M input, $0.42/M output — extremely cheap

**Batching strategy:**
- MAX_CHARS_PER_BATCH = 8000 source chars per batch
- With 8K output tokens (~32K chars), 8000 source chars translates comfortably within limit
- ZZYL chapters: 66-100+ paragraphs, short each (classical Chinese) — usually 1-3 batches
- De Ceremoniis chapters: 2-68 paragraphs, VERY long each (Greek) — needs 5-19 batches
- Chapter paragraph/char sizes:
  - Ceremonialis ch1: 2 para, 2K chars
  - Ceremonialis ch2: 68 para, 150K chars (largest!)
  - Ceremonialis ch3: 9 para, 41K chars → 7 batches
  - Ceremonialis ch4: 9 para, 54K chars
  - Ceremonialis ch5: 7 para, 39K chars
  - Ceremonialis ch6: 6 para, 46K chars
  - Ceremonialis ch7: 16 para, 122K chars

**Translation status (current session):**
- ZZYL ch1: DONE (66 paragraphs, DeepSeek R1)
- ZZYL ch2: DONE (107 paragraphs, DeepSeek V3.2, 4 batches)
- ZZYL ch3: DONE (82 paragraphs, DeepSeek V3.2, 12 batches @ 1500 chars/batch)
- Ceremonialis ch1: DONE (2 paragraphs, DeepSeek R1)
- Ceremonialis ch3: DONE (9 paragraphs, DeepSeek V3.2, 7 batches @ 3000→6000 chars)
- Language-aware batching solved the truncation issues (zh=1500, grc=6000)

**API comparison for translation:**
| Provider | Model | Context | Max Output | Cost (in/out per M) |
|----------|-------|---------|-----------|---------------------|
| DeepSeek | deepseek-chat (V3.2) | 128K | 8K | $0.28 / $0.42 |
| DeepSeek | deepseek-reasoner | 128K | 64K | $0.28 / $0.42 |
| Kimi | kimi-k2 (0905) | 256K | ~8K | $0.60 / $2.50 |
| Claude | Haiku 4.5 | 200K | 8K (64K ext) | $1.00 / $5.00 |
| Claude | Sonnet 4.5 | 1M | 8K (64K ext) | $3.00 / $15.00 |

**Decision:** Using deepseek-chat for cost. Batching handles 8K output limit.

**Language-specific density observation:**
- Classical Chinese (zh): VERY dense. 1 char ≈ 3-5 English words. 3000 source chars → 8K+ output tokens.
- Ancient Greek (grc): Moderate. 1 char ≈ 0.5-1 English words. 3000 source chars → ~2-3K output tokens.
- Latin (la): Similar to Greek.
- TODO: Make MAX_CHARS_PER_BATCH language-aware (e.g., zh=3000, grc=8000, la=8000)

### Database Setup (Session 2)

- Neon PostgreSQL (cloud, free tier)
- Schema pushed via `pnpm db:push` (drizzle-kit push)
- .env.local must be sourced before drizzle-kit commands: `set -a && source .env.local && set +a`
- Database seeded: 4 languages, 2 authors, 2 texts, 147 chapters (140 ZZYL + 7 Ceremonialis)
- Dev server works: pages load with correct data from DB
- Docker NOT available on this system — Neon is the path forward

### Code Changes (Session 2)

- Replaced `@anthropic-ai/sdk` with `openai` package (DeepSeek uses OpenAI-compatible API)
- `src/server/translation/client.ts` → OpenAI client with baseURL "https://api.deepseek.com"
- `scripts/translate-batch.ts` → uses `openai.chat.completions.create()`, model "deepseek-chat"
- Added paragraph chunking (MAX_CHARS_PER_BATCH = 8000)
- Added robust parsing: coerces string indices to numbers
- `.env.example` → DEEPSEEK_API_KEY replaces ANTHROPIC_API_KEY
- `.gitignore` → added API_keys_DO_NOT_COMMIT.txt

### Deployment Plan (Phase 8)

**GitHub remote:** https://github.com/translorentz/translation-wiki.git
**Current state:** 8 commits (7 ahead of origin), plus uncommitted DeepSeek migration

**Steps to go live:**
1. Commit DeepSeek migration changes
2. Push all commits to GitHub
3. User: Connect GitHub repo to Vercel (vercel.com → import project)
4. User: Set environment variables on Vercel:
   - DATABASE_URL = same Neon connection string
   - AUTH_SECRET = generate with `openssl rand -base64 32`
   - AUTH_URL = production URL (e.g., https://translation-wiki.vercel.app)
   - DEEPSEEK_API_KEY = same key (for any server-side translation features)
5. Deploy triggers automatically on push
6. Schema already applied to Neon (step already done)
7. Optional: Custom domain in Vercel dashboard

**Known issues to address:**
- Next.js 16 warns: "middleware" file convention deprecated, use "proxy" instead
  - This is non-blocking (still works), but should be migrated eventually
- NextAuth is beta (5.0.0-beta.30) — monitor for stable release
- CLAUDE.md still references @anthropic-ai/sdk — cosmetic, not blocking
- Only 5/147 chapters have translations (3 ZZYL + 2 Ceremonialis)
  - MVP is fine — shows the structure, more can be added later

**Production environment variables needed:**
| Variable | Source | Notes |
|----------|--------|-------|
| DATABASE_URL | Neon dashboard | Already configured in .env.local |
| AUTH_SECRET | `openssl rand -base64 32` | Must generate for production |
| AUTH_URL | Your Vercel domain | e.g., https://translation-wiki.vercel.app |
| DEEPSEEK_API_KEY | platform.deepseek.com | Same as dev |

### Commands Cheat Sheet

```bash
# Project setup
pnpm create next-app@latest translation-wiki --typescript --tailwind --eslint --app --src-dir
cd translation-wiki
pnpm add @trpc/server @trpc/client @trpc/tanstack-react-query @tanstack/react-query zod superjson drizzle-orm postgres next-auth@beta
pnpm add -D drizzle-kit @types/node vitest @vitejs/plugin-react jsdom @testing-library/react @testing-library/dom @testing-library/jest-dom vite-tsconfig-paths playwright @playwright/test
npx shadcn@latest init
npx shadcn@latest add button card input label table tabs dialog dropdown-menu

# Database
docker compose up -d  # starts local PostgreSQL
pnpm drizzle-kit generate  # generate migration from schema
pnpm drizzle-kit migrate  # apply migration
pnpm drizzle-kit studio  # GUI at localhost:4983

# Development
pnpm dev  # starts Next.js dev server with Turbopack
pnpm build  # production build
pnpm test  # vitest in watch mode
pnpm test run  # vitest single run
```

### Session 3: Local Verification (2026-01-23)

**Issue found:** Schema had `composition_year` and `composition_era` columns in code but not in the Neon database.
**Fix:** Ran `pnpm db:push` to sync schema. All pages now work.

**Verification results — all pages working:**
- Home (`/`): 200 — title "Deltoi", renders featured texts
- Browse texts (`/texts`): 200 — lists texts
- Text index (`/zh/zhu-xi/zhuziyulei`): 200 — chapter listing
- Chapter view (ZZYL ch1): 200 — Chinese content + translation rendered
- Chapter view (Ceremonialis ch1): 200 — Greek content rendered
- Edit page: 307 redirect to login (correct — protected)
- Login: 200
- Register: 200
- Search: 200
- Admin/users: 307 redirect to login (correct — protected)

**Uncommitted work still pending:**
- 66 ZZYL processed chapter files updated
- 7 Ceremonialis processed chapter files updated
- New text: chuanxilu (傳習錄) data + processing script
- Admin pages, home components, schema/auth/trpc changes
- `fix-chapter-titles.ts`, `process-chuanxilu.ts` scripts
- New `src/types/` directory

**Actions taken (continued):**
- Committed all pending changes: b50ba01 (104 files, +3748/-168 lines)
- Pushed to GitHub: b2d49af..b50ba01 main → main
- Vercel auto-deployed successfully (deployment status: success)
- Verified deltoi.com serves the new code (title "Deltoi", all pages 200)
- Created reference-guide.txt — structured quick reference for all sessions
- Updated CLAUDE.md: added session files section, fixed Anthropic→DeepSeek refs,
  added current deployment info, added Chuanxilu to initial texts list

**Discovered:** AUTH_URL env var on Vercel still set to translation-wiki.vercel.app
  (visible in Set-Cookie header). User needs to update this in Vercel dashboard.

**Next steps:**
- User: Update AUTH_URL on Vercel to https://deltoi.com
- Seed chuanxilu into the database
- Run more AI translations (most chapters still untranslated)
- Address middleware deprecation warning (non-blocking)

### Session 4: New Texts Processing & Full Translation (2026-01-23)

**New raw texts discovered in data/raw/:**
1. `deanima_cassiodorus/De_anima.txt` — Latin, 76KB, Cassiodorus "De Anima" (c. 540)
2. `elegia/elegia.txt` — Latin, 43KB, Henry of Settimello "Elegia" (c. 1190)
3. `lombards_of_b/langabardorum.txt` — Latin, 90KB, Erchempert "Historia Langobardorum Beneventanorum" (c. 889)
4. `regno/regno_sicile.txt` — Latin, 263KB, Hugo Falcandus "Liber de Regno Sicilie" (c. 1169)
5. `tongjian_jishi_benmo/` — Chinese, 6.4MB (45 files), Yuan Shu "通鑑紀事本末" (c. 1174)

**Processing completed:**
- Created scripts/process-new-texts.ts — processes all 4 non-trivial texts
- De Anima: 18 chapters (Roman numeral headers, paragraphs by blank lines)
- Elegia: 4 chapters/books (processed by earlier agent, verse grouped into paragraphs)
- Lombards: 82 sections (numbered, mostly single-paragraph sections)
- Regno: 56 chapters (1 prologue + 55 Roman numeral chapters, paragraphs by blank lines)
- Tongjian: 45 chapters (2 prefaces + 42 volumes + 1 afterword, deduplicated paragraphs)

**Database seeded:**
- 6 new authors: Wang Yangming, Cassiodorus, Henry of Settimello, Erchempert, Hugo Falcandus, Yuan Shu
- 6 new texts: Chuanxilu, De Anima, Elegia, Lombards, Regno, Tongjian
- 208 new chapters total (3 + 18 + 4 + 82 + 56 + 45)
- All pages return 200 on localhost:3000

**Translation progress (7 parallel background processes running):**
- De Anima (la, 18 ch): IN PROGRESS
- Elegia (la, 4 ch): IN PROGRESS
- Lombards (la, 82 ch): IN PROGRESS
- Regno (la, 56 ch): IN PROGRESS
- Tongjian (zh, 45 ch): IN PROGRESS — very large volumes, dense Chinese
- ZZYL (zh, 137 remaining): IN PROGRESS — 13+ batches per chapter
- Ceremonialis (grc, 5 remaining): IN PROGRESS — massive chapters (150K chars each)

**Translation API:** Using DeepSeek V3.2 (deepseek-chat), $0.28/$0.42 per M tokens
**All 7 processes running with 0 errors so far.**

**Known issue with Tongjian raw data:** Many paragraphs appear duplicated (each paragraph
repeated twice consecutively). Processing script deduplicates these automatically.

**Files created/modified this session:**
- scripts/process-new-texts.ts (NEW) — processes De Anima, Lombards, Regno, Tongjian
- data/processed/deanima/ (18 files)
- data/processed/lombards/ (82 files)
- data/processed/regno/ (56 files)
- data/processed/tongjian/ (45 files)
- data/processed/elegia/ (4 files, from earlier agent)

**User permissions granted (Session 4):**
- Always allowed to write/update logs, scratchpads, markdowns without asking
- Always allowed to run monitoring bash commands (sleep, tail, curl checks) without user input

**Next steps:**
- Wait for all 7 translation processes to complete
- Commit all new processed data + scripts
- Push to GitHub for Vercel auto-deploy
- Update reference-guide.txt with final translation counts

### Session 5: Poetry Display, HDNJ Processing, Parallel Translation (2026-01-23)

**Poetry display mode implemented:**
- Added `textType` field to texts table schema (varchar, "prose" | "poetry")
- Ptochoprodromos text set to textType="poetry" in seed-db.ts
- InterlinearViewer: accepts `textType` prop, passes `isPoetry` to ParagraphPair
- ParagraphPair: poetry mode uses:
  - Line numbers every 5th line (absolute positioned, tabular-nums)
  - Compact spacing (py-0.5 instead of py-3)
  - No row separators (border removed)
  - Left padding (pl-10 md:pl-12) for line number gutter
  - Header alignment padding matches content
- Chapter page passes `textData.textType` to InterlinearViewer
- Build verified, curl tested (correct CSS classes present)
- Committed as 241a6bf

**Ptochoprodromos translation:**
- Chapter 1 already had translation
- Chapter 2 translated successfully (poetry/verse text)
- First attempt killed by `| head -10` SIGPIPE — re-ran without pipe

**ZZYL translation parallelized:**
- Previous: 1 sequential worker (killed at ch 57/140)
- New: 4 parallel workers with --delay 4000:
  - Worker 1: ch 58-78
  - Worker 2: ch 79-99
  - Worker 3: ch 100-120
  - Worker 4: ch 121-140
- translate-batch.ts skips already-translated chapters (checks currentVersionId)

**Huang Di Nei Jing (黃帝內經) processing:**
- Found 55 raw files in data/raw/huang_di_nei_jing/ (Su Wen section only)
- File format: NNN_素問_Su_Wen_Title.txt
- Each file: title line, === separator, then alternating section headers + content
- Created scripts/process-huangdi.ts:
  - Strips === separators and section headers (lines ending with :)
  - Extracts content paragraphs with 1-based indexing
  - Output: data/processed/huangdineijing/chapter-NNN.json (55 files)
- Added to seed-db.ts:
  - Author: "Huangdi (Traditional Attribution)", slug "huangdi"
  - Text: "Huang Di Nei Jing (The Yellow Emperor's Classic of Medicine)", slug "huangdineijing"
  - Language: zh, compositionYear: -200

**HDNJ title scope fix:**
- User said HDNJ will be 81+81 chapters (Su Wen + Ling Shu) so title shouldn't be just Su Wen
- Updated seed-db.ts: title="Huang Di Nei Jing (The Yellow Emperor's Classic of Medicine)"
- Updated titleOriginalScript from "黃帝內經素問" to "黃帝內經"
- Updated description to mention both sections
- Fixed in database via temporary script _fix-hdnj-title.ts
- Note: inline `tsx -e` failed due to esbuild not supporting `!` non-null assertion

**HDNJ translation started (3 parallel workers):**
- Worker 1: ch 1-18
- Worker 2: ch 19-37
- Worker 3: ch 38-55
- All with --delay 4000

**Discussion pages plan created:**
- Plan file: ~/.claude/plans/radiant-spinning-scone.md
- New tables: discussion_threads, discussion_posts
- tRPC router: discussions.ts (listByChapter, getThread, createThread, createPost, toggleResolved, togglePinned)
- Pages: /[chapter]/discussion/ and /[chapter]/discussion/[threadId]
- Components: ThreadList, ThreadView, CreateThreadForm, PostReplyForm
- All logged-in users can create/reply; admins can pin

**Background translation status (end of session):**
- be044fa: Tongjian (in progress, batch 35/42 on current chapter)
- b6a3898: ZZYL 58-78 (in progress, batch 19/20)
- b0ab038: ZZYL 79-99 (in progress, batch 10/15)
- bd76b8e: ZZYL 100-120 (completed ch 101-102)
- b7ea0d0: ZZYL 121-140 (completed ch 123)
- bd051ef: HDNJ 1-18 (completed ch 9-13)
- bb4c2cb: HDNJ 19-37 (completed ch 26-30)
- b503646: HDNJ 38-55 (completed ch 47-51)

**Errors this session:**
- `| head -10` kills translation script with SIGPIPE — never pipe long-running scripts
- `tsx -e` inline code can't use `!` non-null assertion — use temp script file instead
- Created SCRATCH.md when convention is scratch-notes.txt — deleted via git rm

**Files created this session:**
- scripts/process-huangdi.ts (NEW)
- data/processed/huangdineijing/ (55 chapter files)

**Files modified this session:**
- src/components/interlinear/InterlinearViewer.tsx (textType/isPoetry prop)
- src/components/interlinear/ParagraphPair.tsx (poetry display mode)
- src/app/(wiki)/[lang]/[author]/[text]/[chapter]/page.tsx (passes textType)
- scripts/seed-db.ts (added HDNJ author/text, textType for ptochoprodromos)
- src/server/db/schema.ts (added textType column to texts table)

**Carmina Graeca Medii Aevi — FULLY COMPLETE (raw files + processed JSON done):**
- Raw .txt files: 21 files in data/raw/carmina-graeca/, total 10,957 lines
- Quality: Grade A (Critic Agent evaluation after all fixes)
- See docs/carmina-graeca-scratchpad.md for full agent notes
- See docs/carmina-graeca-critiques.md for per-chapter quality evaluations

**Carmina Graeca Medii Aevi — Implementation Details:**
- File: data/difficult_extra_processing/carmina_graeca_medii_aevi_multiple_titles.txt
- 1.4MB, 23,770 lines of OCR from W. Wagner's 1874 edition
- Contains 21 distinct medieval Greek poems/narratives in one file
- Pipeline: scripts/lib/carmina/ (6 modules + orchestrator)
  - title-finder.ts: 21 hardcoded title positions + heuristic ALL-CAPS detector
  - text-splitter.ts: splits file into 21 sections by title boundaries
  - content-classifier.ts: classifies lines as verse/apparatus/noise/intro/empty
  - verse-extractor.ts: extracts clean verse, strips numbers, groups into 15-line paragraphs
  - quality-checker.ts: validates output with thresholds
  - output-formatter.ts: writes standard chapter JSON
- Key insight: medieval Greek verse NEVER contains Arabic numerals
  - Any embedded digit (not at line start) = apparatus
  - This is the most powerful discriminator since Latin editorial text is OCR'd into Greek Unicode
- Classifier patterns for apparatus detection:
  - hasEmbeddedNumbers(): strips leading editorial number, checks for remaining digits
  - MULTIPLE_NUMBERS: 3+ numbers on one line (standalone, no sigla needed)
  - EDITORIAL_BRACKETS: [ ] { } never appear in verse
  - CODEX_ABBREV: οοα/οοά (= Latin "cod." rendered as Greek)
  - LATIN_IN_GREEK: known Latin-in-Greek-script patterns (Βατγβίαπ = "Bursian", etc.)
  - VERSE_REF: ν. + digit pattern
  - Extended sigla set: Α, Β, Μ, Ρ, Δ, Κ, Υ, Ν, Τ
- Results: PASS=13, WARN=8, FAIL=0, 10,952 verse lines, 1,270 paragraphs
- Contamination rate: 0.055% (6 lines, all OCR artifacts not real apparatus)
- CONTENT_END_LINE fixed from 21200 to 22885 (Belisarius poem extends to line 22882)
- Chapter 21 ending verified: final verse "οὐδὲ θεάσονταί ποτε ὅσα κ᾿ ἂν τραγῳδοῦσιν" present
- Output: data/processed/carmina-graeca/chapter-001.json through chapter-021.json
- seed-db.ts updated: added Wagner as author, Carmina Graeca as text (poetry type)
- Pipeline COMPLETE — ready for database seeding when DATABASE_URL is available
- Background translation workers still running (ZZYL 66+, 117+)
- Tongjian translation: chapters 1-15 done, chapters 16-45 being translated by 2 new workers
  - Old workers killed (had JSON parse errors, single-retry only)
  - translate-batch.ts improved: translateBatchWithRetry() — 3 retries + batch splitting + placeholder fallback
  - Worker 1 (b28bb83): chapters 16-30
  - Worker 2 (b10363e): chapters 31-45

**Discussion pages verification:**
- Schema, router, components, client wrappers, pages all exist and build cleanly
- Confirmed in `pnpm build` output: routes listed correctly
- Schema tables already synced to Neon (db:push returned "No changes detected")

**HDNJ translation complete:**
- All 3 workers finished with 0 errors
- 55 chapters (Su Wen) fully translated
