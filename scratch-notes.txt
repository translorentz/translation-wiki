# Scratch Notes — Translation Wiki Project Planning

## Research Summary

### Tech Stack Confirmed
- Next.js 15+ (App Router, Turbopack) — `npx create-next-app@latest --typescript --tailwind --eslint --app --src-dir`
- tRPC v11 with @trpc/tanstack-react-query — fetch adapter for App Router API routes
- Drizzle ORM — relations v1 API (stable), v2 in beta
- NextAuth.js v5 (Auth.js) — credentials provider + OAuth, JWT strategy
- Shadcn UI — `npx shadcn@latest init` then add components as needed
- Vitest + @testing-library/react — jsdom environment, vite-tsconfig-paths
- PostgreSQL — Neon (free tier: 0.5GB, 190 compute hours) or local Docker

### Key Technical Decisions

1. **Package manager:** pnpm recommended (better shadcn compatibility with React 19)
2. **tRPC v11 pattern:**
   - /trpc folder with init.ts, router.ts, client.tsx, server.tsx
   - fetchRequestHandler at /api/trpc/[trpc]/route.ts
   - createCaller for server components (no HTTP overhead)
   - prefetch + HydrateClient for streaming
3. **Auth pattern:**
   - auth.config.ts (providers) + auth.ts (adapter + session strategy)
   - app/api/auth/[...nextauth]/route.ts exports { GET, POST }
   - JWT strategy (no database session table needed initially)
   - Middleware for route protection
4. **Drizzle pattern:**
   - pgTable definitions with explicit column types
   - relations() for ORM-level relationships
   - .references() for FK constraints
   - drizzle-kit for migrations (drizzle.config.ts)
5. **Testing:**
   - Vitest + jsdom for unit/component tests
   - Playwright for E2E (async server components can't be tested with Vitest)
   - vitest.config.mts at root

### Source Text Details

**Zhu Zi Yu Lei (朱子語類)**
- 140 chapters (juan/卷)
- URN: ctp:zhuzi-yulei
- Individual chapters: ctp:zhuzi-yulei/1 through /140
- Each chapter returns `fulltext` (ordered paragraph list)
- Topics: metaphysics, learning, Four Books commentary, classics, history, misc
- API requires authentication for subsections enumeration
- Python `ctext` package available, or raw JSON API calls

**De Ceremoniis**
- 2 books, ~180+ chapters total
- Internet Archive: archive.org/details/bub_gb_OFpFAAAAYAAJ (Reiske edition)
- Greek text + Latin translation in same edition
- OCR quality varies — polytonic Greek diacritics are problematic
- Princeton Byzantine Translations may have partial cleaner text
- Moffatt/Tall 2012 English translation is under copyright — cannot use
- Need AI-generated English translation from the Greek

### Data Model Notes

Content storage: JSON column with paragraph arrays
- Source: { paragraphs: [{ index: 0, text: "..." }, ...] }
- Translation mirrors with matching indices
- This enables paragraph-aligned rendering in InterlinearViewer
- For Chinese: each "paragraph" might be a single statement or exchange
- For Greek: each "paragraph" might be a section or stanza

Versioning:
- TranslationVersion is append-only
- Each version stores full content (not diffs) — simplifies reading, costs more storage
- Diffs computed on-the-fly for display using a library like `diff` or `jsdiff`
- currentVersionId on Translation points to latest

Endorsements:
- One per user per version (unique constraint)
- Count aggregated for display
- Version with most endorsements = "community preferred"

### Interlinear Viewer Design Notes

Core concept: CSS Grid or Flexbox table with two columns
- Each row = one ParagraphPair component
- Left cell: source text (potentially with original script font)
- Right cell: translation text
- Rows auto-height to tallest cell (natural CSS grid behaviour)
- Synchronized scroll NOT needed if using row-based alignment
- Mobile: stack vertically (source above, translation below)
- Font considerations:
  - Chinese: Noto Serif CJK or Source Han Serif
  - Greek: Noto Serif with polytonic support
  - Latin: any good serif
  - English translation: system font stack or Inter

### Deployment Architecture

MVP path (Vercel + Neon):
- Vercel Hobby (free) handles Next.js deployment
- Neon free tier for PostgreSQL
- Vercel automatically builds on git push
- Environment variables in Vercel dashboard
- Custom domain via Vercel DNS or external registrar

Production path (self-hosted):
- Docker Compose: Next.js container + PostgreSQL container
- Caddy or nginx as reverse proxy with auto-SSL
- VPS: Hetzner CAX11 (€3.79/mo, ARM) or DigitalOcean $6/mo
- Backups: pg_dump cron to object storage

### Implementation Order (Dependencies)

Phase 1 must come first: schema, auth, Docker compose
Phase 2 can parallel with Phase 3 partially
Phase 3 depends on seeded data (Phase 2)
Phase 4 depends on Phase 3 (reading UI exists)
Phase 5 depends on Phase 4 (versions exist to endorse)
Phase 6 can happen any time after Phase 2
Phase 7 can happen after Phase 3
Phase 8 can happen after Phase 1 (deploy empty, then iterate)

Actually, earliest useful deployment is after Phase 3 — reading works.

### Risks and Mitigations

1. ctext.org rate limits / auth requirements
   - Mitigation: create account, cache aggressively, store raw in data/raw/
   - Fallback: manually download chapter pages and parse HTML

2. OCR quality of De Ceremoniis
   - Mitigation: Princeton Byzantine Translations for partial clean text
   - Mitigation: AI-assisted cleanup of OCR output
   - Mitigation: start with a few chapters, iterate

3. Paragraph alignment between source and translation
   - Mitigation: editor enforces paragraph count matching
   - Mitigation: allow "empty" paragraphs for padding

4. Polytonic Greek rendering
   - Mitigation: test with Noto Serif early, specify @font-face
   - Mitigation: Unicode normalization (NFC) on input

5. Scale of Zhu Zi Yu Lei (140 chapters, potentially thousands of paragraphs)
   - Mitigation: paginate by chapter (already planned)
   - Mitigation: lazy-load translation versions

### Translation API Observations (Session 2)

**DeepSeek R1 (deepseek-reasoner):**
- Very slow — each batch takes 30-60+ seconds due to chain-of-thought reasoning
- Unnecessary for translation (reasoning overhead provides no benefit)
- 64K max output — would eliminate batching need, but too slow to be practical
- Sometimes returns string indices instead of numbers (needed coercion fix)

**DeepSeek V3.2 (deepseek-chat):**
- Much faster — responses in seconds
- 8K max output (hard limit enforced by API: "valid range is [1, 8192]")
- Requires paragraph batching for large chapters
- $0.28/M input, $0.42/M output — extremely cheap

**Batching strategy:**
- MAX_CHARS_PER_BATCH = 8000 source chars per batch
- With 8K output tokens (~32K chars), 8000 source chars translates comfortably within limit
- ZZYL chapters: 66-100+ paragraphs, short each (classical Chinese) — usually 1-3 batches
- De Ceremoniis chapters: 2-68 paragraphs, VERY long each (Greek) — needs 5-19 batches
- Chapter paragraph/char sizes:
  - Ceremonialis ch1: 2 para, 2K chars
  - Ceremonialis ch2: 68 para, 150K chars (largest!)
  - Ceremonialis ch3: 9 para, 41K chars → 7 batches
  - Ceremonialis ch4: 9 para, 54K chars
  - Ceremonialis ch5: 7 para, 39K chars
  - Ceremonialis ch6: 6 para, 46K chars
  - Ceremonialis ch7: 16 para, 122K chars

**Translation status (current session):**
- ZZYL ch1: DONE (66 paragraphs, DeepSeek R1)
- ZZYL ch2: DONE (107 paragraphs, DeepSeek V3.2, 4 batches)
- ZZYL ch3: DONE (82 paragraphs, DeepSeek V3.2, 12 batches @ 1500 chars/batch)
- Ceremonialis ch1: DONE (2 paragraphs, DeepSeek R1)
- Ceremonialis ch3: DONE (9 paragraphs, DeepSeek V3.2, 7 batches @ 3000→6000 chars)
- Language-aware batching solved the truncation issues (zh=1500, grc=6000)

**API comparison for translation:**
| Provider | Model | Context | Max Output | Cost (in/out per M) |
|----------|-------|---------|-----------|---------------------|
| DeepSeek | deepseek-chat (V3.2) | 128K | 8K | $0.28 / $0.42 |
| DeepSeek | deepseek-reasoner | 128K | 64K | $0.28 / $0.42 |
| Kimi | kimi-k2 (0905) | 256K | ~8K | $0.60 / $2.50 |
| Claude | Haiku 4.5 | 200K | 8K (64K ext) | $1.00 / $5.00 |
| Claude | Sonnet 4.5 | 1M | 8K (64K ext) | $3.00 / $15.00 |

**Decision:** Using deepseek-chat for cost. Batching handles 8K output limit.

**Language-specific density observation:**
- Classical Chinese (zh): VERY dense. 1 char ≈ 3-5 English words. 3000 source chars → 8K+ output tokens.
- Ancient Greek (grc): Moderate. 1 char ≈ 0.5-1 English words. 3000 source chars → ~2-3K output tokens.
- Latin (la): Similar to Greek.
- TODO: Make MAX_CHARS_PER_BATCH language-aware (e.g., zh=3000, grc=8000, la=8000)

### Database Setup (Session 2)

- Neon PostgreSQL (cloud, free tier)
- Schema pushed via `pnpm db:push` (drizzle-kit push)
- .env.local must be sourced before drizzle-kit commands: `set -a && source .env.local && set +a`
- Database seeded: 4 languages, 2 authors, 2 texts, 147 chapters (140 ZZYL + 7 Ceremonialis)
- Dev server works: pages load with correct data from DB
- Docker NOT available on this system — Neon is the path forward

### Code Changes (Session 2)

- Replaced `@anthropic-ai/sdk` with `openai` package (DeepSeek uses OpenAI-compatible API)
- `src/server/translation/client.ts` → OpenAI client with baseURL "https://api.deepseek.com"
- `scripts/translate-batch.ts` → uses `openai.chat.completions.create()`, model "deepseek-chat"
- Added paragraph chunking (MAX_CHARS_PER_BATCH = 8000)
- Added robust parsing: coerces string indices to numbers
- `.env.example` → DEEPSEEK_API_KEY replaces ANTHROPIC_API_KEY
- `.gitignore` → added API_keys_DO_NOT_COMMIT.txt

### Deployment Plan (Phase 8)

**GitHub remote:** https://github.com/translorentz/translation-wiki.git
**Current state:** 8 commits (7 ahead of origin), plus uncommitted DeepSeek migration

**Steps to go live:**
1. Commit DeepSeek migration changes
2. Push all commits to GitHub
3. User: Connect GitHub repo to Vercel (vercel.com → import project)
4. User: Set environment variables on Vercel:
   - DATABASE_URL = same Neon connection string
   - AUTH_SECRET = generate with `openssl rand -base64 32`
   - AUTH_URL = production URL (e.g., https://translation-wiki.vercel.app)
   - DEEPSEEK_API_KEY = same key (for any server-side translation features)
5. Deploy triggers automatically on push
6. Schema already applied to Neon (step already done)
7. Optional: Custom domain in Vercel dashboard

**Known issues to address:**
- Next.js 16 warns: "middleware" file convention deprecated, use "proxy" instead
  - This is non-blocking (still works), but should be migrated eventually
- NextAuth is beta (5.0.0-beta.30) — monitor for stable release
- CLAUDE.md still references @anthropic-ai/sdk — cosmetic, not blocking
- Only 5/147 chapters have translations (3 ZZYL + 2 Ceremonialis)
  - MVP is fine — shows the structure, more can be added later

**Production environment variables needed:**
| Variable | Source | Notes |
|----------|--------|-------|
| DATABASE_URL | Neon dashboard | Already configured in .env.local |
| AUTH_SECRET | `openssl rand -base64 32` | Must generate for production |
| AUTH_URL | Your Vercel domain | e.g., https://translation-wiki.vercel.app |
| DEEPSEEK_API_KEY | platform.deepseek.com | Same as dev |

### Commands Cheat Sheet

```bash
# Project setup
pnpm create next-app@latest translation-wiki --typescript --tailwind --eslint --app --src-dir
cd translation-wiki
pnpm add @trpc/server @trpc/client @trpc/tanstack-react-query @tanstack/react-query zod superjson drizzle-orm postgres next-auth@beta
pnpm add -D drizzle-kit @types/node vitest @vitejs/plugin-react jsdom @testing-library/react @testing-library/dom @testing-library/jest-dom vite-tsconfig-paths playwright @playwright/test
npx shadcn@latest init
npx shadcn@latest add button card input label table tabs dialog dropdown-menu

# Database
docker compose up -d  # starts local PostgreSQL
pnpm drizzle-kit generate  # generate migration from schema
pnpm drizzle-kit migrate  # apply migration
pnpm drizzle-kit studio  # GUI at localhost:4983

# Development
pnpm dev  # starts Next.js dev server with Turbopack
pnpm build  # production build
pnpm test  # vitest in watch mode
pnpm test run  # vitest single run
```

### Session 3: Local Verification (2026-01-23)

**Issue found:** Schema had `composition_year` and `composition_era` columns in code but not in the Neon database.
**Fix:** Ran `pnpm db:push` to sync schema. All pages now work.

**Verification results — all pages working:**
- Home (`/`): 200 — title "Deltoi", renders featured texts
- Browse texts (`/texts`): 200 — lists texts
- Text index (`/zh/zhu-xi/zhuziyulei`): 200 — chapter listing
- Chapter view (ZZYL ch1): 200 — Chinese content + translation rendered
- Chapter view (Ceremonialis ch1): 200 — Greek content rendered
- Edit page: 307 redirect to login (correct — protected)
- Login: 200
- Register: 200
- Search: 200
- Admin/users: 307 redirect to login (correct — protected)

**Uncommitted work still pending:**
- 66 ZZYL processed chapter files updated
- 7 Ceremonialis processed chapter files updated
- New text: chuanxilu (傳習錄) data + processing script
- Admin pages, home components, schema/auth/trpc changes
- `fix-chapter-titles.ts`, `process-chuanxilu.ts` scripts
- New `src/types/` directory

**Actions taken (continued):**
- Committed all pending changes: b50ba01 (104 files, +3748/-168 lines)
- Pushed to GitHub: b2d49af..b50ba01 main → main
- Vercel auto-deployed successfully (deployment status: success)
- Verified deltoi.com serves the new code (title "Deltoi", all pages 200)
- Created reference-guide.txt — structured quick reference for all sessions
- Updated CLAUDE.md: added session files section, fixed Anthropic→DeepSeek refs,
  added current deployment info, added Chuanxilu to initial texts list

**Discovered:** AUTH_URL env var on Vercel still set to translation-wiki.vercel.app
  (visible in Set-Cookie header). User needs to update this in Vercel dashboard.

**Next steps:**
- User: Update AUTH_URL on Vercel to https://deltoi.com
- Seed chuanxilu into the database
- Run more AI translations (most chapters still untranslated)
- Address middleware deprecation warning (non-blocking)

### Session 4: New Texts Processing & Full Translation (2026-01-23)

**New raw texts discovered in data/raw/:**
1. `deanima_cassiodorus/De_anima.txt` — Latin, 76KB, Cassiodorus "De Anima" (c. 540)
2. `elegia/elegia.txt` — Latin, 43KB, Henry of Settimello "Elegia" (c. 1190)
3. `lombards_of_b/langabardorum.txt` — Latin, 90KB, Erchempert "Historia Langobardorum Beneventanorum" (c. 889)
4. `regno/regno_sicile.txt` — Latin, 263KB, Hugo Falcandus "Liber de Regno Sicilie" (c. 1169)
5. `tongjian_jishi_benmo/` — Chinese, 6.4MB (45 files), Yuan Shu "通鑑紀事本末" (c. 1174)

**Processing completed:**
- Created scripts/process-new-texts.ts — processes all 4 non-trivial texts
- De Anima: 18 chapters (Roman numeral headers, paragraphs by blank lines)
- Elegia: 4 chapters/books (processed by earlier agent, verse grouped into paragraphs)
- Lombards: 82 sections (numbered, mostly single-paragraph sections)
- Regno: 56 chapters (1 prologue + 55 Roman numeral chapters, paragraphs by blank lines)
- Tongjian: 45 chapters (2 prefaces + 42 volumes + 1 afterword, deduplicated paragraphs)

**Database seeded:**
- 6 new authors: Wang Yangming, Cassiodorus, Henry of Settimello, Erchempert, Hugo Falcandus, Yuan Shu
- 6 new texts: Chuanxilu, De Anima, Elegia, Lombards, Regno, Tongjian
- 208 new chapters total (3 + 18 + 4 + 82 + 56 + 45)
- All pages return 200 on localhost:3000

**Translation progress (7 parallel background processes running):**
- De Anima (la, 18 ch): IN PROGRESS
- Elegia (la, 4 ch): IN PROGRESS
- Lombards (la, 82 ch): IN PROGRESS
- Regno (la, 56 ch): IN PROGRESS
- Tongjian (zh, 45 ch): IN PROGRESS — very large volumes, dense Chinese
- ZZYL (zh, 137 remaining): IN PROGRESS — 13+ batches per chapter
- Ceremonialis (grc, 5 remaining): IN PROGRESS — massive chapters (150K chars each)

**Translation API:** Using DeepSeek V3.2 (deepseek-chat), $0.28/$0.42 per M tokens
**All 7 processes running with 0 errors so far.**

**Known issue with Tongjian raw data:** Many paragraphs appear duplicated (each paragraph
repeated twice consecutively). Processing script deduplicates these automatically.

**Files created/modified this session:**
- scripts/process-new-texts.ts (NEW) — processes De Anima, Lombards, Regno, Tongjian
- data/processed/deanima/ (18 files)
- data/processed/lombards/ (82 files)
- data/processed/regno/ (56 files)
- data/processed/tongjian/ (45 files)
- data/processed/elegia/ (4 files, from earlier agent)

**User permissions granted (Session 4):**
- Always allowed to write/update logs, scratchpads, markdowns without asking
- Always allowed to run monitoring bash commands (sleep, tail, curl checks) without user input

**Next steps:**
- Wait for all 7 translation processes to complete
- Commit all new processed data + scripts
- Push to GitHub for Vercel auto-deploy
- Update reference-guide.txt with final translation counts
